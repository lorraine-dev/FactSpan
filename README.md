# FactSpan Dataset Expansion

This repository contains the FactSpan dataset, an extension of the X-Fact dataset, designed to support multilingual fact-checking research. It includes tools to expand and update the dataset with recent claims from the ClaimReview Markup for Data Commons Feed.

## Dataset Overview
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15084388.svg)](https://doi.org/10.5281/zenodo.15084388)

The FactSpan dataset addresses limitations in existing multilingual fact-checking datasets by incorporating recent data and providing detailed annotations. The dataset includes:

-   Claims from both the X-Fact dataset (up to 2020) and the Data Commons Feed (post-2020).
-   Claims filtered from organizations recognized by the International Fact-Checking Network (IFCN) and Duke Reporters’ Lab, ensuring high reliability.
-   Standardized verdict labels (False, Mostly False, Partly False/Misleading, Mostly True, True) for consistency.
-   Rich annotations:
    - Topic Extraction (Health and Pandemics, Politics and Governance, etc.)
    - Claim Type Identification (factual or opinion)
    - Additional key features (Numerical Claims, Quotes, Position Statements, Entity/Event Properties)

## Repository Structure
```
.
├── Data
│   ├── FactSpan.csv               # Original FactSpan dataset.
│   ├── FactSpan_annotated.csv     # FactSpan dataset with annotations.
│   └── ValidFactCheckers          # Lists of valid fact-checking organizations.
│       ├── duke_list.txt
│       └── ifcn_list.txt
├── LICENSE
├── README.md
├── requirements.txt
└── scripts
    ├── annotation                 # Scripts for annotating the dataset.
    │   ├── annotate_factspan.py  # Main annotation script.
    │   ├── topic_extraction.py
    │   ├── claim_type_extraction.py
    │   ├── feature_extraction.py
    │   └── logs                  # Annotation logs.
    └── expansion                  # Scripts for expanding the dataset.
        ├── data_processing.py    # Data processing utilities.
        ├── fact_checker_utils.py # Fact-checker validation utilities.
        ├── update_dataset.py     # Script to update the dataset.
        ├── verdict_mapping.json  # Mapping of verdict labels.
        └── verdict_prefix_mapping.json # Mapping of verdict prefix labels.
```

## Valid Fact-Checking Organizations

The `ValidFactCheckers` directory contains lists of fact-checking organizations used to filter claims in the dataset, ensuring high reliability.

-   **`ifcn_list.txt`:** This file contains a list of fact-checking organizations that are verified signatories of the International Fact-Checking Network (IFCN). The list is directly sourced from the IFCN's GitHub repository: [https://github.com/IFCN/verified-signatories/blob/main/list](https://github.com/IFCN/verified-signatories/blob/main/list).
-   **`duke_list.txt`:** This file contains a list of active fact-checking organizations identified by the Duke Reporters’ Lab. As they do not provide a direct list, this file was generated by web scraping their website to extract the currently active fact-checkers.

## Setting Up the Computational Environment

To ensure proper execution of the scripts in this repository, follow these steps to set up your computational environment.

### Prerequisites

* **Python 3.10 or higher:** This project was developed and tested using Python 3.10. While it might be compatible with other Python 3 versions, we recommend using Python 3.10 or a more recent version to avoid potential compatibility issues. You can check your Python version by running:
    ```bash
    python --version
    ```
    Ensure the output is `Python 3.10.x` or higher.

* **pip:** Python's package installer. It usually comes bundled with Python installations. You can check if it's installed by running:
    ```bash
    pip --version
    ```

### Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/lorraine-dev/FactSpan.git](https://github.com/lorraine-dev/FactSpan.git)
    cd FactSpan
    ```

2.  **Create a virtual environment:**
    It is highly recommended to use a virtual environment to isolate the project's dependencies.
    ```bash
    python -m venv venv
    ```
    This command creates a directory named `venv` in your project directory.

3.  **Activate the virtual environment:**
    Activate the virtual environment before installing dependencies. The activation command depends on your operating system:

    * **On macOS and Linux:**
        ```bash
        source venv/bin/activate
        ```

    * **On Windows (Command Prompt):**
        ```bash
        venv\Scripts\activate
        ```

    * **On Windows (PowerShell):**
        ```powershell
        .\venv\Scripts\Activate.ps1
        ```

    Once activated, you should see `(venv)` at the beginning of your terminal prompt.

4.  **Install the project dependencies:**
    Navigate to the root directory of the project (if you're not already there) and run the following command to install all the required packages listed in the `requirements.txt` file:
    ```bash
    pip install -r requirements.txt
    ```
    This command will install the specific versions of the libraries that this project depends on.

## Dataset Update Instructions

### Option 1: Expanding the Unannotated Dataset (FactSpan.csv)

To expand the `FactSpan.csv` dataset with new claims, follow these steps:

1.  Ensure you have followed the steps in the "Setting Up the Computational Environment" section.
2.  Navigate to the `scripts/expansion` directory:
    ```bash
    cd scripts/expansion
    ```
3.  Run the `update_dataset.py` script, specifying the path to `FactSpan.csv`:
    ```bash
    python update_dataset.py ../../Data/FactSpan.csv
    ```

This will update the `FactSpan.csv` file with the latest claims from the Data Commons Feed.

### Option 2: Expanding the Annotated Dataset (FactSpan_annotated.csv)

To expand the `FactSpan_annotated.csv` dataset, which requires LLM-based annotations, you need to configure your OpenAI API token:

1.  Ensure you have followed the steps in the "Setting Up the Computational Environment" section.
2.  Add your OpenAI API token to a `.env` file in the root directory. Create the `.env` file if it doesn't exist. Add the following line to the file, replacing `<your_openai_token>` with your actual token:
    ```
    OPENAI_API_KEY=<your_openai_token>
    ```
3.  Navigate to the `scripts/expansion` directory (if you are not already there).
4.  Run the `update_dataset.py` script with the `--annotations` flag, specifying the path to `FactSpan_annotated.csv`:
    ```bash
    python update_dataset.py ../../Data/FactSpan_annotated.csv --annotations
    ```

This will update the `FactSpan_annotated.csv` file with new claims and their corresponding annotations.

## Dataset DOI

The FactSpan dataset is available on Zenodo:

* **Latest Version (Concept DOI):** [10.5281/zenodo.15084387](https://doi.org/10.5281/zenodo.15084387) (This DOI always points to the most recent version of the dataset.)
* **Version 1.0.0:** [10.5281/zenodo.15084388](https://doi.org/10.5281/zenodo.15084388) (This DOI points to the specific version 1.0.0 of the dataset.)

**Note:** The `.env` file and the `scripts/annotation/logs` and `scripts/expansion/logs` directories are ignored by Git, as specified in the `.gitignore` file.

---

This work was partially funded by the German Research Foundation (DFG) under project No. 504226141.