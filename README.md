# FactSpan Dataset Expansion

This repository contains the FactSpan dataset, an extension of the X-Fact dataset, designed to support multilingual fact-checking research. It includes tools to expand and update the dataset with recent claims from the ClaimReview Markup for Data Commons Feed.

## Dataset Overview
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15084388.svg)](https://doi.org/10.5281/zenodo.15084388)

The FactSpan dataset addresses limitations in existing multilingual fact-checking datasets by incorporating recent data and providing detailed annotations. The dataset includes:

-   Claims from both the X-Fact dataset (up to 2020) and the Data Commons Feed (post-2020).
-   Claims filtered from organizations recognized by the International Fact-Checking Network (IFCN) and Duke Reporters’ Lab, ensuring high reliability.
-   Standardized verdict labels (False, Mostly False, Partly False/Misleading, Mostly True, True) for consistency.
-   Rich annotations:
    - Topic Extraction (Health and Pandemics, Politics and Governance, etc.)
    - Claim Type Identification (factual or opinion)
    - Additional key features (Numerical Claims, Quotes, Position Statements, Entity/Event Properties)

## Repository Structure
```
.
├── Data
│   ├── FactSpan.csv               # Original FactSpan dataset.
│   ├── FactSpan_annotated.csv     # FactSpan dataset with annotations.
│   └── ValidFactCheckers          # Lists of valid fact-checking organizations.
│       ├── duke_list.txt
│       └── ifcn_list.txt
├── LICENSE
├── README.md
├── requirements.txt
└── scripts
    ├── annotation                 # Scripts for annotating the dataset.
    │   ├── annotate_factspan.py  # Main annotation script.
    │   ├── topic_extraction.py
    │   ├── claim_type_extraction.py
    │   ├── feature_extraction.py
    │   └── logs                  # Annotation logs.
    └── expansion                  # Scripts for expanding the dataset.
        ├── data_processing.py    # Data processing utilities.
        ├── fact_checker_utils.py # Fact-checker validation utilities.
        ├── update_dataset.py     # Script to update the dataset.
        ├── verdict_mapping.json  # Mapping of verdict labels.
        └── verdict_prefix_mapping.json # Mapping of verdict prefix labels.
```

## Valid Fact-Checking Organizations

The `ValidFactCheckers` directory contains lists of fact-checking organizations used to filter claims in the dataset, ensuring high reliability.

-   **`ifcn_list.txt`:** This file contains a list of fact-checking organizations that are verified signatories of the International Fact-Checking Network (IFCN). The list is directly sourced from the IFCN's GitHub repository: [https://github.com/IFCN/verified-signatories/blob/main/list](https://github.com/IFCN/verified-signatories/blob/main/list).
-   **`duke_list.txt`:** This file contains a list of active fact-checking organizations identified by the Duke Reporters’ Lab. As they do not provide a direct list, this file was generated by web scraping their website to extract the currently active fact-checkers.

## Dataset Update Instructions

### Option 1: Expanding the Unannotated Dataset (FactSpan.csv)

To expand the `FactSpan.csv` dataset with new claims, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/lorraine-dev/FactSpan.git
   cd FactSpan
   ```
2.  Navigate to the `scripts/expansion` directory:
    ```bash
    cd scripts/expansion
    ```
3.  Run the `update_dataset.py` script, specifying the path to `FactSpan.csv`:
    ```bash
    python update_dataset.py ../../Data/FactSpan.csv
    ```

This will update the `FactSpan.csv` file with the latest claims from the Data Commons Feed.

### Option 2: Expanding the Annotated Dataset (FactSpan_annotated.csv)

To expand the `FactSpan_annotated.csv` dataset, which requires LLM-based annotations, you need to configure your OpenAI API token:

1.  Add your OpenAI API token to a `.env` file in the root directory. Create the `.env` file if it doesn't exist. Add the following line to the file, replacing `<your_openai_token>` with your actual token:
    ```
    OPENAI_API_KEY=<your_openai_token>
    ```
2.  Clone the repository and navigate to the `scripts/expansion` directory (as in Option 1).
3.  Run the `update_dataset.py` script with the `--annotations` flag, specifying the path to `FactSpan_annotated.csv`:
    ```bash
    python update_dataset.py ../../Data/FactSpan_annotated.csv --annotations
    ```

This will update the `FactSpan_annotated.csv` file with new claims and their corresponding annotations.
## Dataset DOI

The FactSpan dataset is available on Zenodo:

* **Latest Version (Concept DOI):** [10.5281/zenodo.15084387](https://doi.org/10.5281/zenodo.15084387) (This DOI always points to the most recent version of the dataset.)
* **Version 1.0.0:** [10.5281/zenodo.15084388](https://doi.org/10.5281/zenodo.15084388) (This DOI points to the specific version 1.0.0 of the dataset.)

**Note:** The `.env` file and the `scripts/annotation/logs` and `scripts/expansion/logs` directories are ignored by Git, as specified in the `.gitignore` file.